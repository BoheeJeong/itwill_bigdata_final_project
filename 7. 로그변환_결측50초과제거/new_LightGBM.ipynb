{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LightGBM (LGBMClassifier) 분석 (독립 실행용)\n",
        "\n",
        "데이터 로딩부터 성능평가·과적합 확인·SHAP·결과 저장까지 수행합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 라이브러리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pandas import DataFrame\n",
        "try:\n",
        "    from IPython.display import display\n",
        "except ImportError:\n",
        "    display = print\n",
        "from matplotlib import pyplot as plt\n",
        "plt.ioff()\n",
        "import seaborn as sb\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        "    roc_curve,\n",
        "    log_loss,\n",
        ")\n",
        "import shap\n",
        "import os\n",
        "\n",
        "my_dpi = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, r'C:\\\\itwill_bigdata_final_project-main\\\\itwill_bigdata_final_project\\\\3. 결측 변수 제거 없이 분석 진행')\n",
        "\n",
        "# 2. 로지스틱+성능평가+shap copy에서 정의한 함수를 공통 모듈에서 import (다른 ipynb에서도 동일하게 사용 가능)\n",
        "from analysis_utils import (\n",
        "    hs_get_scores, hs_describe, category_describe, hs_feature_importance,\n",
        "    create_figure, finalize_plot, hs_learning_cv, hs_get_score_cv,\n",
        "    my_shap_analysis, hs_shap_dependence_analysis, my_dpi,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 데이터 로딩 (2. 로지스틱+성능평가+shap copy와 동일)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pandas import read_csv\n",
        "\n",
        "origin = read_csv(\n",
        "    r'C:\\itwill_bigdata_final_project-main\\itwill_bigdata_final_project\\1. 초기 데이터 전처리\\3.coding_book_mapping.csv',\n",
        "    encoding='utf-8'\n",
        ")\n",
        "origin.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "categorical_cols = [\n",
        "    'w09_fam1','w09_fam2','w09edu','w09gender1','w09marital','w09edu_s','w09ecoact_s','w09enu_type',\n",
        "    'w09ba069','w09bp1','w09c152','w09c001','w09c003','w09c005',\n",
        "    'w09chronic_a','w09chronic_b','w09chronic_c','w09chronic_d','w09chronic_e','w09chronic_f',\n",
        "    'w09chronic_g','w09chronic_h','w09chronic_i','w09chronic_j','w09chronic_k','w09chronic_l','w09chronic_m',\n",
        "    'w09c056','w09c068','w09c081','w09c082','w09c085','w09c102',\n",
        "    'w09smoke','w09alc','w09addic','w09c550',\n",
        "    'w09f001type','w09g031',\n",
        "    'w09cadd_19','w09c142','w09c143','w09c144','w09c145','w09c146','w09c147','w09c148','w09c149','w09c150','w09c151'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "origin_type_changed = origin.copy()\n",
        "cat_cols_for_type = [c for c in categorical_cols if c in origin_type_changed.columns]\n",
        "origin_type_changed[cat_cols_for_type] = origin_type_changed[cat_cols_for_type].astype(\"category\")\n",
        "origin = origin_type_changed.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "origin2 = origin.drop(['dependent_wage_work'], axis=1)\n",
        "yname = \"dependent_ecotype\"\n",
        "# 결측치가 50% 초과인 변수 제거 (타깃 제외)\n",
        "missing_rate = origin2.isnull().mean()\n",
        "drop_high_missing = [c for c in missing_rate[missing_rate > 0.5].index if c != yname]\n",
        "origin3 = origin2.drop(columns=drop_high_missing)\n",
        "print(f'결측 50% 초과 변수 제거: {len(drop_high_missing)}개 제거, 남은 컬럼 {origin3.shape[1]}개')\n",
        "df2 = origin3.copy()\n",
        "df3 = df2.copy()\n",
        "\n",
        "drop_for_leakage = [yname, 'work_ability_age']\n",
        "x = df3.drop(columns=[c for c in drop_for_leakage if c in df3.columns])\n",
        "y = df3[yname].astype(int)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    x, y, test_size=0.25, random_state=52, stratify=y\n",
        ")\n",
        "x_train.shape, x_test.shape, y_train.shape, y_test.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## #01 컬럼 타입 분리\n",
        "\n",
        "**전처리 기준:** `2. 로지스틱 + 성능평가 + shap copy.ipynb` 파이프라인과 동일\n",
        "- **연속형** (int64, float64): 결측 → median 대체 후 StandardScaler\n",
        "- **명목형** (object, category): 결측 → 'Missing' 대체 후 OneHotEncoder(drop='first', handle_unknown='ignore')\n",
        "- 컬럼 구분: train 기준 `select_dtypes`로 자동 분리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. 로지스틱+성능평가+shap copy와 동일한 컬럼 타입 분리 기준\n",
        "cat_cols = x_train.select_dtypes(include=[\"object\", \"category\"]).columns\n",
        "num_cols = x_train.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
        "print(\"categorical:\", len(cat_cols))\n",
        "print(\"numeric:\", len(num_cols))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## #02 전처리 + LightGBM 파이프라인 & GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 로그 변환 적용: 금액/자산/소득 등 연속형 일부에 log1p 적용\n",
        "LOG_COLS_CANDIDATES = ['w09earned', 'w09pinc', 'w09e201', 'w09e207', 'w09e213', 'w09e219', 'w09e225', 'w09e231', 'w09e237', 'w09e243', 'w09e273', 'w09e251', 'w09passets', 'w09pliabilities', 'w09pnetassets', 'w09hhinc', 'w09hhassets', 'w09hhliabilities', 'w09hhnetassets', 'w09fromchildren', 'w09tochildren', 'w09transferfrom', 'w09transferto']\n",
        "log_cols = [c for c in LOG_COLS_CANDIDATES if c in num_cols.tolist()]\n",
        "other_num_cols = [c for c in num_cols if c not in log_cols]\n",
        "\n",
        "num_log_pipe = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"log\", FunctionTransformer(np.log1p)),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "num_other_pipe = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_pipe = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"Missing\")),\n",
        "    (\"onehot\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\"))\n",
        "])\n",
        "\n",
        "transformers_list = []\n",
        "if len(log_cols) > 0:\n",
        "    transformers_list.append((\"num_log\", num_log_pipe, log_cols))\n",
        "if len(other_num_cols) > 0:\n",
        "    transformers_list.append((\"num_other\", num_other_pipe, other_num_cols))\n",
        "transformers_list.append((\"cat\", categorical_pipe, cat_cols))\n",
        "\n",
        "preprocess = ColumnTransformer(transformers=transformers_list)\n",
        "\n",
        "pipe = Pipeline([\n    (\"imputer\", SimpleImputer(strategy=\"median\")),\n    (\"scaler\", StandardScaler())\n])\n\ncategorical_pipe = Pipeline([\n    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"Missing\")),\n    (\"onehot\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\"))\n])\n\npreprocess = ColumnTransformer(\n    transformers=[\n        (\"num\", numeric_pipe, num_cols),\n        (\"cat\", categorical_pipe, cat_cols),\n    ]\n)\n\npipe = Pipeline([\n    (\"preprocess\", preprocess),\n    (\"model\", LGBMClassifier(random_state=52, verbose=-1))\n])\n\nparam_grid = {\n    \"model__n_estimators\": [100, 200],\n    \"model__max_depth\": [3, 5, 7],\n    \"model__learning_rate\": [0.05, 0.1],\n    \"model__num_leaves\": [31, 63],\n    \"model__class_weight\": [None, \"balanced\"],\n}\n\ngs = GridSearchCV(\n    estimator=pipe,\n    param_grid=param_grid,\n    cv=5,\n    scoring=\"roc_auc\",\n    n_jobs=-1\n)\n\ngs.fit(x_train, y_train)\nestimator = gs.best_estimator_\n\nprint(\"Best CV AUC:\", gs.best_score_)\nprint(\"Best params:\", gs.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## #03 예측값"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = estimator.predict(x_test)\n",
        "y_pred_proba = estimator.predict_proba(x_test)\n",
        "y_pred_proba_1 = y_pred_proba[:, 1]\n",
        "y_pred[:5], y_pred_proba_1[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## #04 성능 평가"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_test, y_pred)\n",
        "((TN, FP), (FN, TP)) = cm\n",
        "print(TN, FP, FN, TP)\n",
        "\n",
        "cmdf = DataFrame(\n",
        "    cm,\n",
        "    index=['Actual 0 (TN/FP)', 'Actual 1 (FN/TP)'],\n",
        "    columns=['Predicted (Negative)', 'Predicted (Positive)']\n",
        ")\n",
        "display(cmdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "width_px, height_px = 800, 600\n",
        "fig, ax = plt.subplots(1, 1, figsize=(width_px / my_dpi, height_px / my_dpi), dpi=my_dpi)\n",
        "sb.heatmap(data=cmdf, annot=True, fmt=\"d\", linewidth=0.5, cmap=\"PuOr\")\n",
        "ax.set_xlabel(\"\")\n",
        "ax.set_ylabel(\"\")\n",
        "ax.xaxis.tick_top()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "tpr = recall_score(y_test, y_pred)\n",
        "fpr = FP / (TN + FP)\n",
        "tnr = 1 - fpr\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "auc = roc_auc_score(y_test, y_pred_proba_1)\n",
        "\n",
        "y_null = np.ones_like(y_test) * y_test.mean()\n",
        "log_loss_test = -log_loss(y_test, y_pred_proba, normalize=False)\n",
        "log_loss_null = -log_loss(y_test, y_null, normalize=False)\n",
        "pseudo_r2 = 1 - (log_loss_test / log_loss_null)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall(TPR):\", tpr)\n",
        "print(\"FPR:\", fpr)\n",
        "print(\"TNR:\", tnr)\n",
        "print(\"F1:\", f1)\n",
        "print(\"AUC:\", auc)\n",
        "print(\"Pseudo R2:\", pseudo_r2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ROC 곡선"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "roc_fpr, roc_tpr, _ = roc_curve(y_test, y_pred_proba_1)\n",
        "fig, ax = plt.subplots(1, 1, figsize=(1000 / my_dpi, 900 / my_dpi), dpi=my_dpi)\n",
        "sb.lineplot(x=roc_fpr, y=roc_tpr)\n",
        "sb.lineplot(x=[0, 1], y=[0, 1], color='red', linestyle=\":\", alpha=0.5)\n",
        "plt.fill_between(x=roc_fpr, y1=roc_tpr, alpha=0.1)\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.set_title(f\"AUC={auc:.4f}\", fontsize=10, pad=4)\n",
        "ax.set_xlim(0, 1)\n",
        "ax.set_ylim(0, 1)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 결과표"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if hasattr(estimator, \"named_steps\"):\n",
        "    classname = estimator.named_steps[\"model\"].__class__.__name__\n",
        "else:\n",
        "    classname = estimator.__class__.__name__\n",
        "\n",
        "score_df = DataFrame(\n",
        "    {\n",
        "        \"의사결정계수(R2)\": [round(pseudo_r2, 3)],\n",
        "        \"정확도(Accuracy)\": [round(accuracy, 3)],\n",
        "        \"정밀도(Precision)\": [round(precision, 3)],\n",
        "        \"재현율(Recall)\": [round(tpr, 3)],\n",
        "        \"위양성율(Fallout)\": [round(fpr, 3)],\n",
        "        \"특이성(TNR)\": [round(tnr, 3)],\n",
        "        \"F1 Score\": [round(f1, 3)],\n",
        "        \"AUC\": [round(auc, 3)],\n",
        "    },\n",
        "    index=[classname]\n",
        ")\n",
        "score_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## #05 Learning Curve & 과적합 판정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_sizes = np.linspace(0.1, 1.0, 10)\n",
        "sizes, train_scores, cv_scores = learning_curve(\n",
        "    estimator=estimator,\n",
        "    X=x_train,\n",
        "    y=y_train.astype(int),\n",
        "    train_sizes=train_sizes,\n",
        "    cv=5,\n",
        "    scoring=\"roc_auc\",\n",
        "    n_jobs=-1,\n",
        "    shuffle=True,\n",
        "    random_state=52\n",
        ")\n",
        "\n",
        "train_mean = train_scores.mean(axis=1)\n",
        "cv_mean = cv_scores.mean(axis=1)\n",
        "cv_std = cv_scores.std(axis=1)\n",
        "final_train = train_mean[-1]\n",
        "final_cv = cv_mean[-1]\n",
        "final_std = cv_std[-1]\n",
        "gap_ratio = final_train - final_cv\n",
        "\n",
        "print(\"Final Train AUC:\", final_train)\n",
        "print(\"Final CV AUC:\", final_cv)\n",
        "print(\"Final CV STD:\", final_std)\n",
        "print(\"Gap(Train-CV):\", gap_ratio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if final_train < 0.6 and final_cv < 0.6:\n",
        "    status = \"⚠ 과소적합\"\n",
        "elif gap_ratio > 0.1:\n",
        "    status = \"⚠ 과대적합\"\n",
        "elif gap_ratio <= 0.05 and final_std <= 0.05:\n",
        "    status = \"✅ 일반화 양호\"\n",
        "elif final_std > 0.1:\n",
        "    status = \"⚠ 데이터 부족\"\n",
        "else:\n",
        "    status = \"⚠ 판단 보류\"\n",
        "\n",
        "result_df = DataFrame(\n",
        "    {\n",
        "        \"Train ROC_AUC 평균\": [round(final_train, 3)],\n",
        "        \"CV ROC_AUC 평균\": [round(final_cv, 3)],\n",
        "        \"CV ROC_AUC 표준편차\": [round(final_std, 3)],\n",
        "        \"Train/CV 비율\": [round(gap_ratio, 3)],\n",
        "        \"CV 변동성 비율\": [round(final_std, 3)],\n",
        "        \"판정 결과\": [status],\n",
        "    },\n",
        "    index=[classname],\n",
        ")\n",
        "result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(1600 / my_dpi, 960 / my_dpi), dpi=my_dpi)\n",
        "sb.lineplot(x=train_sizes, y=train_mean, marker=\"o\", markeredgecolor=\"#ffffff\", label=\"Train ROC_AUC\")\n",
        "sb.lineplot(x=train_sizes, y=cv_mean, marker=\"o\", markeredgecolor=\"#ffffff\", label=\"CV ROC_AUC\")\n",
        "ax.fill_between(train_sizes, train_mean - train_scores.std(axis=1), train_mean + train_scores.std(axis=1), alpha=0.1)\n",
        "ax.fill_between(train_sizes, cv_mean - cv_std, cv_mean + cv_std, alpha=0.1)\n",
        "ax.set_xlabel(\"Train size\")\n",
        "ax.set_ylabel(\"ROC_AUC\")\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## #06 SHAP (LightGBM TreeExplainer)\n",
        "\n",
        "전처리된 데이터와 내부 모델(LGBMClassifier)로 TreeExplainer 적용."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_transformed = estimator.named_steps[\"preprocess\"].transform(x_train)\n",
        "feature_names = estimator.named_steps[\"preprocess\"].get_feature_names_out()\n",
        "inner_model = estimator.named_steps[\"model\"]\n",
        "X_train_df = DataFrame(X_train_transformed, columns=feature_names, index=x_train.index)\n",
        "\n",
        "explainer = shap.TreeExplainer(inner_model, data=X_train_df)\n",
        "shap_values = explainer.shap_values(X_train_df)\n",
        "\n",
        "if isinstance(shap_values, list):\n",
        "    shap_values = shap_values[1]\n",
        "\n",
        "shap_df = DataFrame(shap_values, columns=feature_names, index=x_train.index)\n",
        "summary_df = DataFrame(\n",
        "    {\n",
        "        \"feature\": shap_df.columns,\n",
        "        \"mean_abs_shap\": shap_df.abs().mean().values,\n",
        "        \"mean_shap\": shap_df.mean().values,\n",
        "        \"std_shap\": shap_df.std().values,\n",
        "    }\n",
        ")\n",
        "summary_df[\"direction\"] = np.where(\n",
        "    summary_df[\"mean_shap\"] > 0, \"양(+) 경향\",\n",
        "    np.where(summary_df[\"mean_shap\"] < 0, \"음(-) 경향\", \"혼합/미약\")\n",
        ")\n",
        "summary_df = summary_df.sort_values(\"mean_abs_shap\", ascending=False).reset_index(drop=True)\n",
        "total_importance = summary_df[\"mean_abs_shap\"].sum()\n",
        "summary_df[\"importance_ratio\"] = summary_df[\"mean_abs_shap\"] / total_importance\n",
        "summary_df[\"importance_cumsum\"] = summary_df[\"importance_ratio\"].cumsum()\n",
        "summary_df[\"is_important\"] = np.where(summary_df[\"importance_cumsum\"] <= 0.80, \"core\", \"secondary\")\n",
        "display(summary_df.head(20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "shap.summary_plot(shap_values, X_train_df, show=False)\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(16, 8)\n",
        "plt.title(\"SHAP Summary Plot (LightGBM)\", fontsize=10, pad=10)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 결과 저장 (종합.ipynb에서 사용)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "os.makedirs('results', exist_ok=True)\n",
        "save_name = 'new_LightGBM'\n",
        "with open(os.path.join('results', save_name + '.pkl'), 'wb') as f:\n",
        "    pickle.dump({\n",
        "        'model_name': classname,\n",
        "        'score_df': score_df,\n",
        "        'result_df': result_df,\n",
        "        'overfit_status': status,\n",
        "        'estimator': estimator,\n",
        "        'x_train': x_train,\n",
        "        'x_test': x_test,\n",
        "        'y_train': y_train,\n",
        "        'y_test': y_test,\n",
        "        'auc': auc,\n",
        "    }, f)\n",
        "print('Saved results to results/' + save_name + '.pkl')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}